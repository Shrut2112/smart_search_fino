{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260646d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports loaded\n",
      "ğŸ“ PDF_DIR: data\\pdfs\n",
      "ğŸ“ CHUNKS_DIR: data\\chunks\n",
      "ğŸ“„ METADATA: True\n",
      "ğŸ“„ FAQ: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain for chunking & embeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Paths\n",
    "PDF_DIR = Path(\"data/pdfs\")\n",
    "CHUNKS_DIR = Path(\"data/chunks\")\n",
    "METADATA_PATH = Path(\"metadata/canonical/fino_policies_metadata.json\")\n",
    "FAQ_PATH = Path(\"data/raw/fino_full_qa_dump.txt\")  \n",
    "\n",
    "# Create directories\n",
    "PDF_DIR.mkdir(exist_ok=True)\n",
    "CHUNKS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ… Imports loaded\")\n",
    "print(f\"ğŸ“ PDF_DIR: {PDF_DIR}\")\n",
    "print(f\"ğŸ“ CHUNKS_DIR: {CHUNKS_DIR}\")\n",
    "print(f\"ğŸ“„ METADATA: {METADATA_PATH.exists()}\")\n",
    "print(f\"ğŸ“„ FAQ: {FAQ_PATH.exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a63b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Loaded 16 policy versions\n",
      "ğŸ¯ Active policies: 14\n",
      "ğŸ“‚ PDFs in data/pdfs/: 16\n",
      "âœ… All PDFs found\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Load canonical metadata\n",
    "with open(METADATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    policies = json.load(f)\n",
    "\n",
    "print(f\"ğŸ“‹ Loaded {len(policies)} policy versions\")\n",
    "print(f\"ğŸ¯ Active policies: {sum(1 for p in policies if p['status'] == 'Active')}\")\n",
    "print(f\"ğŸ“‚ PDFs in data/pdfs/: {len(list(PDF_DIR.glob('*.pdf')))}\")\n",
    "\n",
    "# Validate all PDF paths exist\n",
    "missing_pdfs = []\n",
    "for policy in policies:\n",
    "    pdf_path = PDF_DIR / policy['file_name']\n",
    "    if not pdf_path.exists():\n",
    "        missing_pdfs.append(policy['file_name'])\n",
    "\n",
    "if missing_pdfs:\n",
    "    print(f\"Missing {len(missing_pdfs)} PDFs:\")\n",
    "    for pdf in missing_pdfs[:5]:\n",
    "        print(f\"   - {pdf}\")\n",
    "else:\n",
    "    print(\"âœ… All PDFs found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "340c7e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Universal Extractor loaded (Layers 1-4)\n"
     ]
    }
   ],
   "source": [
    "def universal_extract(pdf_path: Path) -> Tuple[str, Dict]:\n",
    "    \"\"\"\n",
    "    ROBUST EXTRACTION: Layer 1-4 (KMRL + Tables + Fallback + Cleaning)\n",
    "    Returns (full_text, extraction_stats)\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text_parts = []\n",
    "    extraction_stats = {\n",
    "        \"total_pages\": len(doc),\n",
    "        \"text_blocks\": 0,\n",
    "        \"table_blocks\": 0,\n",
    "        \"raw_chars\": 0\n",
    "    }\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        \n",
    "        # LAYER 1: BLOCKS (text + table structure)\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        page_content = []\n",
    "        \n",
    "        for block in blocks:\n",
    "            x0, y0, x1, y1, block_text, _, block_type = block\n",
    "            if block_text.strip():\n",
    "                if block_type == 0:  # Text block\n",
    "                    page_content.append(block_text)\n",
    "                    extraction_stats[\"text_blocks\"] += 1\n",
    "                else:  # Table/graphics\n",
    "                    rect = fitz.Rect(x0, y0, x1, y1)\n",
    "                    table_text = page.get_text(\"text\", clip=rect)\n",
    "                    if table_text.strip():\n",
    "                        page_content.append(f\"\\n[TABLE]{table_text.strip()}[/TABLE]\\n\")\n",
    "                        extraction_stats[\"table_blocks\"] += 1\n",
    "        \n",
    "        # LAYER 2: TABLES (PyMuPDF table detection)\n",
    "        tables = page.find_tables()\n",
    "        for table in tables:\n",
    "            try:\n",
    "                df = pd.DataFrame(table.extract())\n",
    "                table_str = df.to_string(index=False, header=False)\n",
    "                page_content.append(f\"\\n[TABLE-DATA]\\n{table_str}\\n[/TABLE-DATA]\")\n",
    "                extraction_stats[\"table_blocks\"] += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # LAYER 3: RAW FALLBACK\n",
    "        raw_text = page.get_text()\n",
    "        if raw_text.strip() and len(raw_text) > 50:\n",
    "            page_content.append(raw_text)\n",
    "            extraction_stats[\"raw_chars\"] += len(raw_text)\n",
    "        \n",
    "        # LAYER 4: Aggressive cleaning\n",
    "        page_text = \"\\n\\n\".join(page_content)\n",
    "        page_text = re.sub(r'Classification: Internal.*?(?=\\n{2,}|\\Z)', '', page_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        page_text = re.sub(r'FINO Payments Bank Limited\\s*\\n?', '', page_text, flags=re.IGNORECASE)\n",
    "        page_text = re.sub(r'^\\s*\\d+\\s*\\n?', '', page_text, flags=re.MULTILINE)\n",
    "        page_text = re.sub(r'\\n{5,}', '\\n\\n\\n', page_text)\n",
    "        page_text = re.sub(r'\\s{4,}', ' ', page_text)\n",
    "        \n",
    "        if len(page_text.strip()) > 30:\n",
    "            full_text_parts.append(page_text.strip())\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    full_text = \"\\n\\n=== PAGE BREAK ===\\n\\n\".join(full_text_parts)\n",
    "    extraction_stats[\"total_chars\"] = len(full_text)\n",
    "    extraction_stats[\"total_words\"] = len(full_text.split())\n",
    "    \n",
    "    return full_text, extraction_stats\n",
    "\n",
    "print(\"âœ… Universal Extractor loaded (Layers 1-4)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecde22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BGE-M3 specs\n",
    "# BGE_M3_MAX_TOKENS = 8192  # Context window\n",
    "# BGE_M3_MAX_CHUNK_CHARS = 4000  # Safe embedding limit (~75% of max)\n",
    "# CHUNK_OVERLAP_CHARS = 200  # Semantic overlap\n",
    "\n",
    "# print(f\"ğŸ”§ BGE-M3 Config:\")\n",
    "# print(f\"   Max tokens: {BGE_M3_MAX_TOKENS:,}\")\n",
    "# print(f\"   Max chunk chars: {BGE_M3_MAX_CHUNK_CHARS}\")\n",
    "# print(f\"   Overlap: {CHUNK_OVERLAP_CHARS} chars\")\n",
    "\n",
    "# # Initialize embeddings for semantic chunking\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "# print(\"âœ… BGE-M3 embeddings initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870c3e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name D:/models/e5-large. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ E5-Large Config:\n",
      "   Max tokens: 1,024\n",
      "   Max chunk chars: 2000\n",
      "   Overlap: 200 chars\n",
      "âœ… E5-Large embeddings initialized\n"
     ]
    }
   ],
   "source": [
    "E5_MAX_TOKENS = 1024          # Effective context window for E5\n",
    "E5_MAX_CHUNK_CHARS = 2000     # Safe chunk size for embeddings\n",
    "CHUNK_OVERLAP_CHARS = 200     # Semantic overlap \n",
    "\n",
    "print(f\"ğŸ”§ E5-Large Config:\")\n",
    "print(f\"   Max tokens: {E5_MAX_TOKENS:,}\")\n",
    "print(f\"   Max chunk chars: {E5_MAX_CHUNK_CHARS}\")\n",
    "print(f\"   Overlap: {CHUNK_OVERLAP_CHARS} chars\")\n",
    "\n",
    "# Initialize embeddings for semantic chunking \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"D:/models/e5-large\"   # local multilingual-e5-large\n",
    ")\n",
    "\n",
    "print(\"âœ… E5-Large embeddings initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee05ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing 14 ACTIVE policies...\n",
      "\n",
      "[1/14] ğŸ“„ Comprehensive Deposit Policy - V7 (1).pdf\n",
      "   Policy: Comprehensive Deposit Policy - V7 (1).pdf...\n",
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n",
      "   ğŸ“Š Extraction: 76,348 chars, 11,270 words\n",
      "   ğŸ“Š Blocks: 224 text + 5 tables\n",
      "   âœ‚ï¸  68/88 valid chunks\n",
      "   ğŸ“ Saved: comprehensivedepositpolicy_7.json\n",
      "   ğŸ“ Sizes: 154-1982 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: Classification: Public Comprehensive Deposit Policy V7.0 \n",
      "Page 1 Comprehensive Deposit Policy Version 7.0 The policy as enclosed was last approved by ...\n",
      "\n",
      "[2/14] ğŸ“„ 1736412697_1362025_BSBDAPolicy.pdf\n",
      "   Policy: 1736412697_1362025_BSBDAPolicy.pdf...\n",
      "   ğŸ“Š Extraction: 33,523 chars, 5,138 words\n",
      "   ğŸ“Š Blocks: 185 text + 6 tables\n",
      "   âœ‚ï¸  29/40 valid chunks\n",
      "   ğŸ“ Saved: bsbdapolicy_1.json\n",
      "   ğŸ“ Sizes: 206-1485 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: 4 B. Regulatory reference ................................................................................................... 4 C. Policy content .......\n",
      "\n",
      "[3/14] ğŸ“„ 1733376480_DeceasedClaimSettlementPolicy.pdf\n",
      "   Policy: 1733376480_DeceasedClaimSettlementPolicy.pdf...\n",
      "   ğŸ“Š Extraction: 37,600 chars, 5,505 words\n",
      "   ğŸ“Š Blocks: 95 text + 8 tables\n",
      "   âœ‚ï¸  29/47 valid chunks\n",
      "   ğŸ“ Saved: deceasedclaimsettlementpolicy_1.json\n",
      "   ğŸ“ Sizes: 175-1594 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: FINO Payments Bank Deceased Claim Settlement Policy \n",
      "Version 1.0 Document review and approval July 2024 FINO Payments Bank \n",
      "Deceased Claim Settlement ...\n",
      "\n",
      "[4/14] ğŸ“„ Customer Compensation Policy.pdf\n",
      "   Policy: Customer Compensation Policy.pdf...\n",
      "   ğŸ“Š Extraction: 53,173 chars, 7,909 words\n",
      "   ğŸ“Š Blocks: 153 text + 5 tables\n",
      "   âœ‚ï¸  36/55 valid chunks\n",
      "   ğŸ“ Saved: customercompensationpolicy_1.json\n",
      "   ğŸ“ Sizes: 221-1358 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: FINO Payments Bank Customer Compensation Policy Version 11.0 \n",
      "FINO Payments Bank Customer Compensation Policy Version 11.0\n",
      "\n",
      "=== PAGE BREAK ===\n",
      "\n",
      "FINO P...\n",
      "\n",
      "[5/14] ğŸ“„ Customer Grievance Policy_0.pdf\n",
      "   Policy: Customer Grievance Policy_0.pdf...\n",
      "   ğŸ“Š Extraction: 30,260 chars, 4,351 words\n",
      "   ğŸ“Š Blocks: 149 text + 1 tables\n",
      "   âœ‚ï¸  23/30 valid chunks\n",
      "   ğŸ“ Saved: customergrievancepolicy_1.json\n",
      "   ğŸ“ Sizes: 185-1660 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: FINO Payments Bank Customer Grievance Redressal Policy\n",
      "\n",
      "=== PAGE BREAK ===\n",
      "\n",
      "Table of Contents \n",
      ". Policy Usage Guide .....................................\n",
      "\n",
      "[6/14] ğŸ“„ Code of Banks Commitment-to-Customers_0.pdf\n",
      "   Policy: Code of Banks Commitment-to-Customers_0.pdf...\n",
      "   ğŸ“Š Extraction: 44,713 chars, 6,928 words\n",
      "   ğŸ“Š Blocks: 107 text + 1 tables\n",
      "   âœ‚ï¸  44/58 valid chunks\n",
      "   ğŸ“ Saved: codeofbankscommittmenttocustomers_1.json\n",
      "   ğŸ“ Sizes: 168-1936 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: FINO Payments Bank Code of Bankâ€™s Commitment to  Customers Version 10.0 \n",
      "FINO Payments Bank \n",
      "Code of Bankâ€™s Commitment to  Customers Version 10.0\n",
      "\n",
      "===...\n",
      "\n",
      "[7/14] ğŸ“„ Customer Service Policy_0.pdf\n",
      "   Policy: Customer Service Policy_0.pdf...\n",
      "   ğŸ“Š Extraction: 212,375 chars, 31,705 words\n",
      "   ğŸ“Š Blocks: 535 text + 1 tables\n",
      "   âœ‚ï¸  179/231 valid chunks\n",
      "   ğŸ“ Saved: customerservicepolicy_1.json\n",
      "   ğŸ“ Sizes: 153-1940 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: Fino Payments Bank Customer Service Policy Version 13.0 \n",
      "Fino Payments Bank Customer Service Policy Version 13.0\n",
      "\n",
      "=== PAGE BREAK ===\n",
      "\n",
      "Table of Content...\n",
      "\n",
      "[8/14] ğŸ“„ 1683371692_KYCAMLCFTPolicy.pdf\n",
      "   Policy: 1683371692_KYCAMLCFTPolicy.pdf...\n",
      "   ğŸ“Š Extraction: 116,623 chars, 18,263 words\n",
      "   ğŸ“Š Blocks: 1706 text + 3 tables\n",
      "   âœ‚ï¸  71/100 valid chunks\n",
      "   ğŸ“ Saved: revisedkyc_wef2ndmay2023_1.json\n",
      "   ğŸ“ Sizes: 154-1979 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: KYC / AML / CFT Policy Version 4.0\n",
      "\n",
      "=== PAGE BREAK ===\n",
      "\n",
      "INDEX Chapter\n",
      "Title\n",
      "Page No. I \n",
      "Introduction and applicability \n",
      "II \n",
      "Definitions \n",
      "III \n",
      "Customer...\n",
      "\n",
      "[9/14] ğŸ“„ 1690896825_PrivacyPolicyforMitraapplication.pdf\n",
      "   Policy: 1690896825_PrivacyPolicyforMitraapplication.pdf...\n",
      "   ğŸ“Š Extraction: 36,247 chars, 5,764 words\n",
      "   ğŸ“Š Blocks: 35 text + 0 tables\n",
      "   âœ‚ï¸  39/42 valid chunks\n",
      "   ğŸ“ Saved: privacy_formitraapplication_1.json\n",
      "   ğŸ“ Sizes: 227-1899 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: Customers use of the Application and the Service (a) must strictly be in \n",
      "accordance with this Agreement, and includes only the right to download, ins...\n",
      "\n",
      "[10/14] ğŸ“„ Customer Suitability and Appropriateness_0.pdf\n",
      "   Policy: Customer Suitability and Appropriateness_0.pdf...\n",
      "   ğŸ“Š Extraction: 29,337 chars, 4,073 words\n",
      "   ğŸ“Š Blocks: 71 text + 1 tables\n",
      "   âœ‚ï¸  23/31 valid chunks\n",
      "   ğŸ“ Saved: policyoncustomersuitabilityandappropriateness_1.json\n",
      "   ğŸ“ Sizes: 179-1954 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: FINO Payments Bank Policy on Customer Suitability and\n",
      "\n",
      "\n",
      "Appropriateness Version 10.0 FINO Payments Bank  \n",
      "Policy on Customer Suitability and\n",
      "Appropria...\n",
      "\n",
      "[11/14] ğŸ“„ 1655716940_PolicyforAppointmentofStatutoryauditors.pdf\n",
      "   Policy: 1655716940_PolicyforAppointmentofStatutoryauditors...\n",
      "   ğŸ“Š Extraction: 16,427 chars, 2,667 words\n",
      "   ğŸ“Š Blocks: 44 text + 1 tables\n",
      "   âœ‚ï¸  10/12 valid chunks\n",
      "   ğŸ“ Saved: attached_onappointmentofstatutoryauditors_1.json\n",
      "   ğŸ“ Sizes: 202-1996 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: POLICY FOR A\n",
      "\n",
      "\n",
      "APPOINTMENT OF STA\n",
      "\n",
      "\n",
      "AUDITORS TUTORY POLICY FOR A\n",
      " \n",
      "APPOINTMENT OF STA\n",
      "AUDITORS \n",
      "TUTORY\n",
      "\n",
      "=== PAGE BREAK ===\n",
      "\n",
      "Table of Contents ï‚·\n",
      "Object...\n",
      "\n",
      "[12/14] ğŸ“„ 1749546895_KYCAMLpolicy.pdf\n",
      "   Policy: 1749546895_KYCAMLpolicy.pdf...\n",
      "   ğŸ“Š Extraction: 408,501 chars, 64,000 words\n",
      "   ğŸ“Š Blocks: 2136 text + 11 tables\n",
      "   âœ‚ï¸  207/315 valid chunks\n",
      "   ğŸ“ Saved: kycamlcftpolicy_1.json\n",
      "   ğŸ“ Sizes: 155-1967 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: .2 Directions. Adherences to Know Your Customer (KYC), Anti \n",
      " Money Laundering (AML), and Countering Financing of Terrorism (CFT) Guidelines are few s...\n",
      "\n",
      "[13/14] ğŸ“„ 1744362993_ChequeCollectionPolicy.pdf\n",
      "   Policy: 1744362993_ChequeCollectionPolicy.pdf...\n",
      "   ğŸ“Š Extraction: 24,425 chars, 3,720 words\n",
      "   ğŸ“Š Blocks: 116 text + 3 tables\n",
      "   âœ‚ï¸  14/21 valid chunks\n",
      "   ğŸ“ Saved: chequecollectionpolicy_1.json\n",
      "   ğŸ“ Sizes: 204-1732 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: Cheque Collection Policy V6.0 Cheque Collection Policy INDEX SR \n",
      "No Particulars \n",
      "Page No. Introduction \n",
      "Arrangements for Collection \n",
      "Time Frame for co...\n",
      "\n",
      "[14/14] ğŸ“„ Citizens Charter.pdf\n",
      "   Policy: Citizens Charter.pdf...\n",
      "   ğŸ“Š Extraction: 25,843 chars, 3,456 words\n",
      "   ğŸ“Š Blocks: 144 text + 1 tables\n",
      "   âœ‚ï¸  27/33 valid chunks\n",
      "   ğŸ“ Saved: citizenscharter_1.json\n",
      "   ğŸ“ Sizes: 166-1568 chars\n",
      "--------------------------------------------------------------------------------\n",
      "   ğŸ§ª Sample: Classification: Public FINO Payments Bank Citizenâ€™s Charter Classification: Public FINO Payments Bank \n",
      "Citizenâ€™s Charter\n",
      "\n",
      "=== PAGE BREAK ===\n",
      "\n",
      "Classifi...\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ EXTRACTION + CHUNKING COMPLETE!\n",
      "ğŸ“Š Total chunks: 799\n",
      "ğŸ“ Total chars: 589,221\n",
      "ğŸ“ Chunk files: 14\n",
      "âœ… Extraction report: data/extraction_report.json\n",
      "âœ… All chunks ready for embeddings â†’ data/chunks/\n",
      "\n",
      "ğŸ¯ Next: Cell 5 (Chunk validation + embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Process ALL Active policies (FIXED)\n",
    "active_policies = [p for p in policies if p[\"status\"] == \"Active\"]\n",
    "print(f\"ğŸš€ Processing {len(active_policies)} ACTIVE policies...\\n\")\n",
    "\n",
    "all_chunks = []\n",
    "extraction_summary = []\n",
    "\n",
    "for idx, policy in enumerate(active_policies, 1):\n",
    "    pdf_path = PDF_DIR / policy['file_name']\n",
    "    \n",
    "    # FIXED: Safe policy_name access\n",
    "    policy_name = policy.get('policy_name', policy.get('file_name', 'Unknown Policy'))[:50]\n",
    "    \n",
    "    print(f\"[{idx}/{len(active_policies)}] ğŸ“„ {policy['file_name']}\")\n",
    "    print(f\"   Policy: {policy_name}...\")\n",
    "    \n",
    "    # Universal extraction\n",
    "    full_text, stats = universal_extract(pdf_path)\n",
    "    print(f\"   ğŸ“Š Extraction: {stats['total_chars']:,} chars, {stats['total_words']:,} words\")\n",
    "    print(f\"   ğŸ“Š Blocks: {stats['text_blocks']} text + {stats['table_blocks']} tables\")\n",
    "    \n",
    "    extraction_summary.append({\n",
    "        **policy,\n",
    "        \"policy_name_display\": policy_name,  # Add fallback\n",
    "        \"extraction_stats\": stats\n",
    "    })\n",
    "    \n",
    "    # Semantic chunking with E5-Large (your config)\n",
    "    chunker = SemanticChunker(\n",
    "        embeddings,\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        breakpoint_threshold_amount=85\n",
    "    )\n",
    "    \n",
    "    raw_chunks = chunker.split_text(full_text)\n",
    "    \n",
    "    # Filter + validate chunks (E5 limits)\n",
    "    valid_chunks = []\n",
    "    for i, chunk in enumerate(raw_chunks):\n",
    "        chunk_len = len(chunk)\n",
    "        if 150 <= chunk_len <= E5_MAX_CHUNK_CHARS:  # Your E5 config\n",
    "            valid_chunks.append({\n",
    "                \"policy_id\": policy[\"policy_id\"],\n",
    "                \"version_number\": policy[\"version_number\"],\n",
    "                \"chunk_id\": f\"{policy['policy_id']}_{policy['version_number']}_{i:04d}\",\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk,\n",
    "                \"text_length\": chunk_len,\n",
    "                \"word_count\": len(chunk.split()),\n",
    "                \"metadata\": policy,\n",
    "                \"extraction_stats\": stats\n",
    "            })\n",
    "    \n",
    "    all_chunks.extend(valid_chunks)\n",
    "    \n",
    "    # Save per-policy chunks\n",
    "    policy_chunks_file = CHUNKS_DIR / f\"{policy['policy_id']}_{policy['version_number']}.json\"\n",
    "    with open(policy_chunks_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_chunks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"   âœ‚ï¸  {len(valid_chunks)}/{len(raw_chunks)} valid chunks\")\n",
    "    print(f\"   ğŸ“ Saved: {policy_chunks_file.name}\")\n",
    "    print(f\"   ğŸ“ Sizes: {min(c['text_length'] for c in valid_chunks) if valid_chunks else 0}-\"\n",
    "          f\"{max(c['text_length'] for c in valid_chunks) if valid_chunks else 0} chars\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Sample first chunk\n",
    "    if valid_chunks:\n",
    "        print(f\"   ğŸ§ª Sample: {valid_chunks[0]['text'][:150]}...\")\n",
    "    print()\n",
    "\n",
    "# Global summary\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ‰ EXTRACTION + CHUNKING COMPLETE!\")\n",
    "print(f\"ğŸ“Š Total chunks: {len(all_chunks):,}\")\n",
    "print(f\"ğŸ“ Total chars: {sum(c['text_length'] for c in all_chunks):,}\")\n",
    "print(f\"ğŸ“ Chunk files: {len(list(CHUNKS_DIR.glob('*.json')))}\")\n",
    "\n",
    "# Save extraction report\n",
    "with open(\"data/extraction_report.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(extraction_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Extraction report: data/extraction_report.json\")\n",
    "print(f\"âœ… All chunks ready for embeddings â†’ data/chunks/\")\n",
    "print(f\"\\nğŸ¯ Next: Cell 5 (Chunk validation + embeddings)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4a098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing FAQ file...\n",
      "ğŸ“„ FAQ raw: 88,607 chars, 13,783 words\n",
      "âœ‚ï¸ FAQ: 35 chunks â†’ faq_section_1.0.json\n",
      "\n",
      "ğŸ“Š TOTAL (PDFs + FAQ): 834 chunks\n"
     ]
    }
   ],
   "source": [
    "# FAQ Processing (MERGE WITH PDF CHUNKS)\n",
    "print(\"ğŸ“„ Processing FAQ file...\")\n",
    "\n",
    "if FAQ_PATH.exists():\n",
    "    with open(FAQ_PATH, 'r', encoding='utf-8') as f:\n",
    "        faq_text = f.read()\n",
    "    \n",
    "    print(f\"ğŸ“„ FAQ raw: {len(faq_text):,} chars, {len(faq_text.split()):,} words\")\n",
    "    \n",
    "    # FAQ metadata (simple)\n",
    "    faq_metadata = {\n",
    "        \"policy_id\": \"faq_section\",\n",
    "        \"version_number\": \"1.0\",\n",
    "        \"status\": \"Active\",\n",
    "        \"effective_date_ts\": int(datetime.now().timestamp()),\n",
    "        \"expiry_date_ts\": 4102425000,  # 2100\n",
    "        \"effective_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"expiry_date\": \"2100-01-01\",\n",
    "        \"language\": \"en\",\n",
    "        \"file_name\": FAQ_PATH.name,\n",
    "        \"policy_name\": \"Fino Bank FAQs (Website)\",\n",
    "        \"sr\": \"FAQ-001\"\n",
    "    }\n",
    "    \n",
    "    # Semantic chunk FAQ\n",
    "    faq_chunker = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
    "    faq_raw_chunks = faq_chunker.split_text(faq_text)\n",
    "    \n",
    "    faq_chunks = []\n",
    "    for i, chunk in enumerate(faq_raw_chunks):\n",
    "        chunk_len = len(chunk)\n",
    "        if 150 <= chunk_len <= 4000:\n",
    "            faq_chunks.append({\n",
    "                \"policy_id\": \"faq_section\",\n",
    "                \"version_number\": \"1.0\",\n",
    "                \"chunk_id\": f\"faq_section_1.0_{i:04d}\",\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk,\n",
    "                \"text_length\": chunk_len,\n",
    "                \"word_count\": len(chunk.split()),\n",
    "                \"metadata\": faq_metadata,\n",
    "                \"extraction_stats\": {\n",
    "                    \"total_chars\": len(faq_text),\n",
    "                    \"total_words\": len(faq_text.split()),\n",
    "                    \"source\": \"faq_txt\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Save FAQ chunks\n",
    "    faq_output = CHUNKS_DIR / \"faq_section_1.0.json\"\n",
    "    with open(faq_output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(faq_chunks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    all_chunks.extend(faq_chunks)\n",
    "    print(f\"âœ‚ï¸ FAQ: {len(faq_chunks)} chunks â†’ {faq_output.name}\")\n",
    "    \n",
    "    extraction_summary.append({\n",
    "        **faq_metadata,\n",
    "        \"extraction_stats\": {\n",
    "            \"total_chars\": len(faq_text),\n",
    "            \"total_words\": len(faq_text.split()),\n",
    "            \"source\": \"faq_txt\"\n",
    "        }\n",
    "    })\n",
    "else:\n",
    "    print(\"âš ï¸ FAQ file not found: data/raw/faqs.txt\")\n",
    "\n",
    "print(f\"\\nğŸ“Š TOTAL (PDFs + FAQ): {len(all_chunks):,} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327eeb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Found 15 chunk files\n",
      "Sample chunk files:\n",
      "   attached_onappointmentofstatutoryauditors_1.json: 10 chunks\n",
      "   bsbdapolicy_1.json: 29 chunks\n",
      "   chequecollectionpolicy_1.json: 14 chunks\n",
      "   citizenscharter_1.json: 27 chunks\n",
      "   codeofbankscommittmenttocustomers_1.json: 44 chunks\n",
      "âœ… Loaded 10 chunks from attached_onappointmentofstatutoryauditors_1.json\n",
      "âœ… Loaded 29 chunks from bsbdapolicy_1.json\n",
      "âœ… Loaded 14 chunks from chequecollectionpolicy_1.json\n",
      "âœ… Loaded 27 chunks from citizenscharter_1.json\n",
      "âœ… Loaded 44 chunks from codeofbankscommittmenttocustomers_1.json\n",
      "âœ… Loaded 68 chunks from comprehensivedepositpolicy_7.json\n",
      "âœ… Loaded 36 chunks from customercompensationpolicy_1.json\n",
      "âœ… Loaded 23 chunks from customergrievancepolicy_1.json\n",
      "âœ… Loaded 179 chunks from customerservicepolicy_1.json\n",
      "âœ… Loaded 29 chunks from deceasedclaimsettlementpolicy_1.json\n",
      "âœ… Loaded 35 chunks from faq_section_1.0.json\n",
      "âœ… Loaded 207 chunks from kycamlcftpolicy_1.json\n",
      "âœ… Loaded 23 chunks from policyoncustomersuitabilityandappropriateness_1.json\n",
      "âœ… Loaded 39 chunks from privacy_formitraapplication_1.json\n",
      "âœ… Loaded 71 chunks from revisedkyc_wef2ndmay2023_1.json\n",
      "\n",
      "ğŸ“Š TOTAL: 834 chunks ready for E5 embedding\n",
      "ğŸ“ Chunk sizes: 153 - 3668 chars\n",
      "\n",
      "ğŸ“ˆ Quality stats (E5-Large optimized):\n",
      "   total_chunks: 834\n",
      "   avg_length: 761.2\n",
      "   policies_covered: 15\n",
      "   avg_words: 115.7\n",
      "   max_chunk_chars: 3668\n",
      "   min_chunk_chars: 153\n",
      "\n",
      "âš ï¸  7 chunks exceed E5 limit (2000 chars)\n",
      "\n",
      "âœ… Chunks validated â†’ Ready for E5-Large embeddings\n"
     ]
    }
   ],
   "source": [
    "# Load & Validate All Chunks (E5-Large)\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Verify chunking results first\n",
    "chunk_files = list(CHUNKS_DIR.glob(\"*.json\"))\n",
    "print(f\"ğŸ“ Found {len(chunk_files)} chunk files\")\n",
    "print(\"Sample chunk files:\")\n",
    "for f in chunk_files[:5]:\n",
    "    with open(f, 'r', encoding='utf-8') as ff:\n",
    "        chunks = json.load(ff)\n",
    "    print(f\"   {f.name}: {len(chunks)} chunks\")\n",
    "\n",
    "# Load ALL chunks (PDFs + FAQ)\n",
    "all_policy_chunks = []\n",
    "for chunk_file in sorted(chunk_files):\n",
    "    with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "        policy_chunks = json.load(f)\n",
    "    all_policy_chunks.extend(policy_chunks)\n",
    "    print(f\"âœ… Loaded {len(policy_chunks):,} chunks from {chunk_file.name}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š TOTAL: {len(all_policy_chunks):,} chunks ready for E5 embedding\")\n",
    "print(f\"ğŸ“ Chunk sizes: {min(c['text_length'] for c in all_policy_chunks)} - \"\n",
    "      f\"{max(c['text_length'] for c in all_policy_chunks)} chars\")\n",
    "\n",
    "# Chunk quality check (E5-safe)\n",
    "chunk_stats = {\n",
    "    \"total_chunks\": len(all_policy_chunks),\n",
    "    \"avg_length\": sum(c['text_length'] for c in all_policy_chunks) / len(all_policy_chunks),\n",
    "    \"policies_covered\": len(set(c['policy_id'] for c in all_policy_chunks)),\n",
    "    \"avg_words\": sum(c['word_count'] for c in all_policy_chunks) / len(all_policy_chunks),\n",
    "    \"max_chunk_chars\": max(c['text_length'] for c in all_policy_chunks),\n",
    "    \"min_chunk_chars\": min(c['text_length'] for c in all_policy_chunks)\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Quality stats (E5-Large optimized):\")\n",
    "for k, v in chunk_stats.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"   {k}: {v:.1f}\")\n",
    "    else:\n",
    "        print(f\"   {k}: {v}\")\n",
    "\n",
    "# E5 constraint check\n",
    "unsafe_chunks = [c for c in all_policy_chunks if c['text_length'] > E5_MAX_CHUNK_CHARS]\n",
    "if unsafe_chunks:\n",
    "    print(f\"\\nâš ï¸  {len(unsafe_chunks)} chunks exceed E5 limit ({E5_MAX_CHUNK_CHARS} chars)\")\n",
    "else:\n",
    "    print(f\"\\nâœ… All chunks safe for E5 embedding (max {E5_MAX_CHUNK_CHARS} chars)\")\n",
    "\n",
    "print(\"\\nâœ… Chunks validated â†’ Ready for E5-Large embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0746406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading E5-Large embeddings (multilingual, local)...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:68\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\sentence_transformers\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     13\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     CrossEncoder,\n\u001b[0;32m     17\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     19\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\sentence_transformers\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\sentence_transformers\\backend\\load.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _save_pretrained_wrapper, backend_should_export, backend_warn_to_save\n",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\transformers\\__init__.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     30\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     is_pretty_midi_available,\n\u001b[0;32m     37\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\transformers\\dependency_versions_check.py:57\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\transformers\\utils\\versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[1;34m(requirement)\u001b[0m\n\u001b[0;32m    116\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\transformers\\utils\\versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[1;34m(requirement, hint)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 111\u001b[0m     \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\transformers\\utils\\versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[1;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: huggingface-hub>=0.34.0,<1.0 is required for a normal functioning of this module, but found huggingface-hub==1.2.3.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”„ Loading E5-Large embeddings (multilingual, local)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# E5-Large already loaded from Cell 3\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Verify it's ready\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m e5_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/models/e5-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormalize_embeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Test embedding on first chunk\u001b[39;00m\n\u001b[0;32m     16\u001b[0m test_chunk \u001b[38;5;241m=\u001b[39m all_policy_chunks[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m500\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\risha\\anaconda3\\envs\\fino\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:74\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     70\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m     )\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_optimum_intel_available() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_ipex_available():\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "# Generate E5-Large Dense Embeddings\n",
    "import numpy as np\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"ğŸ”„ Loading E5-Large embeddings (multilingual, local)...\")\n",
    "\n",
    "# E5-Large already loaded from Cell 3\n",
    "# Verify it's ready\n",
    "e5_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"D:/models/e5-large\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Test embedding on first chunk\n",
    "test_chunk = all_policy_chunks[0]['text'][:500]\n",
    "test_embedding = e5_embeddings.embed_query(test_chunk)\n",
    "print(f\"âœ… E5-Large test embedding: {len(test_embedding)} dimensions\")\n",
    "print(f\"   Model: multilingual-e5-large (local)\")\n",
    "print(f\"   Embedding type: Dense (cosine similarity)\")\n",
    "print(f\"   Normalization: L2 normalized\")\n",
    "\n",
    "# Generate embeddings for ALL chunks (BATCHED for efficiency)\n",
    "print(\"\\nğŸš€ Generating E5-Large embeddings for all chunks...\")\n",
    "\n",
    "all_embeddings_data = []\n",
    "batch_size = 32  # E5 batch size for CPU\n",
    "total_batches = (len(all_policy_chunks) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in range(0, len(all_policy_chunks), batch_size):\n",
    "    batch_end = min(batch_idx + batch_size, len(all_policy_chunks))\n",
    "    batch_chunks = all_policy_chunks[batch_idx:batch_end]\n",
    "    \n",
    "    # Extract texts for batch\n",
    "    batch_texts = [c['text'] for c in batch_chunks]\n",
    "    \n",
    "    # E5 embedding (dense only - no sparse for local model)\n",
    "    batch_embeddings = e5_embeddings.embed_documents(batch_texts)\n",
    "    \n",
    "    # Attach embeddings to chunks\n",
    "    for i, (chunk, embedding) in enumerate(zip(batch_chunks, batch_embeddings)):\n",
    "        all_embeddings_data.append({\n",
    "            **chunk,\n",
    "            \"dense_embedding\": embedding,\n",
    "            \"embedding_model\": \"e5-large\",\n",
    "            \"embedding_dims\": len(embedding)\n",
    "        })\n",
    "    \n",
    "    progress = (batch_end / len(all_policy_chunks)) * 100\n",
    "    print(f\"   Progress: {batch_end}/{len(all_policy_chunks)} chunks ({progress:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Generated {len(all_embeddings_data)} E5-Large embeddings\")\n",
    "print(f\"ğŸ“Š Embedding dimension: {len(all_embeddings_data[0]['dense_embedding'])}\")\n",
    "print(f\"ğŸ“Š All embeddings normalized (L2)\")\n",
    "\n",
    "# Save embeddings locally (backup)\n",
    "embeddings_backup = []\n",
    "for data in all_embeddings_data:\n",
    "    embeddings_backup.append({\n",
    "        \"chunk_id\": data['chunk_id'],\n",
    "        \"embedding_dims\": data['embedding_dims'],\n",
    "        \"embedding_model\": data['embedding_model']\n",
    "    })\n",
    "\n",
    "with open(\"data/embeddings_metadata.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(embeddings_backup, f, indent=2)\n",
    "\n",
    "print(\"âœ… Embeddings metadata backup: data/embeddings_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1196d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Pinecone-Ready Vectors (E5-Large)\n",
    "print(\"ğŸ”„ Building Pinecone vectors from E5 embeddings...\")\n",
    "\n",
    "pinecone_vectors = []\n",
    "\n",
    "for data in all_embeddings_data:\n",
    "    # Build metadata dict (banking + temporal critical)\n",
    "    metadata = {\n",
    "        **data['metadata'],  # All original metadata\n",
    "        \"chunk_id\": data['chunk_id'],\n",
    "        \"chunk_index\": data['chunk_index'],\n",
    "        \"text_length\": data['text_length'],\n",
    "        \"word_count\": data['word_count'],\n",
    "        \"embedding_model\": \"e5-large\",\n",
    "        \"text_preview\": data['text'][:200] + \"...\" if len(data['text']) > 200 else data['text']\n",
    "    }\n",
    "    \n",
    "    # Pinecone vector (E5 dense only)\n",
    "    vector = {\n",
    "        \"id\": data['chunk_id'],\n",
    "        \"values\": data['dense_embedding'],  # E5 dense (1024-dim)\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    \n",
    "    pinecone_vectors.append(vector)\n",
    "\n",
    "print(f\"âœ… Built {len(pinecone_vectors)} Pinecone vectors\")\n",
    "print(f\"ğŸ“ Vector dimension: {len(pinecone_vectors[0]['values'])}\")\n",
    "print(f\"ğŸ“Š Metadata keys: {list(pinecone_vectors[0]['metadata'].keys())[:5]}...\")\n",
    "\n",
    "# Sample vector structure\n",
    "print(f\"\\nğŸ§ª Sample vector:\")\n",
    "print(f\"   ID: {pinecone_vectors[0]['id']}\")\n",
    "print(f\"   Dimension: {len(pinecone_vectors[0]['values'])}\")\n",
    "print(f\"   Policy: {pinecone_vectors[0]['metadata']['policy_id']}\")\n",
    "print(f\"   Status: {pinecone_vectors[0]['metadata'].get('status', 'N/A')}\")\n",
    "\n",
    "# Save Pinecone vectors to disk (backup before upload)\n",
    "vectors_backup = []\n",
    "for v in pinecone_vectors:\n",
    "    vectors_backup.append({\n",
    "        \"id\": v['id'],\n",
    "        \"dimension\": len(v['values']),\n",
    "        \"metadata_keys\": list(v['metadata'].keys())\n",
    "    })\n",
    "\n",
    "with open(\"data/pinecone_vectors_structure.json\", 'w') as f:\n",
    "    json.dump(vectors_backup, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Vector structure backup: data/pinecone_vectors_structure.json\")\n",
    "print(f\"âœ… Ready for Pinecone upload\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86061fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone Index (E5-Large config)\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Pinecone config (ADAPT WITH YOUR KEY)\n",
    "PINECONE_API_KEY = \"your-api-key-here\"  # âš ï¸ ADD YOUR KEY\n",
    "PINECONE_INDEX_NAME = \"fino-policies-e5\"  # E5-specific index\n",
    "E5_EMBEDDING_DIM = 1024  # E5-Large dimension\n",
    "\n",
    "# Initialize Pinecone\n",
    "print(f\"ğŸ”„ Initializing Pinecone with API key...\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# List existing indexes\n",
    "print(f\"ğŸ“‹ Existing indexes: {[idx.name for idx in pc.list_indexes()]}\")\n",
    "\n",
    "# Check if index exists\n",
    "if PINECONE_INDEX_NAME not in [idx.name for idx in pc.list_indexes()]:\n",
    "    print(f\"ğŸ”„ Creating Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "    print(f\"   Dimension: {E5_EMBEDDING_DIM}\")\n",
    "    print(f\"   Metric: cosine\")\n",
    "    print(f\"   Type: Serverless\")\n",
    "    \n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        dimension=E5_EMBEDDING_DIM,\n",
    "        metric=\"cosine\",  # E5 uses cosine similarity\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    print(f\"âœ… Index created: {PINECONE_INDEX_NAME}\")\n",
    "else:\n",
    "    print(f\"âœ… Index exists: {PINECONE_INDEX_NAME}\")\n",
    "\n",
    "# Connect to index\n",
    "index = pc.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "# Get index stats\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nğŸ“Š Index stats:\")\n",
    "print(f\"   Total vectors: {stats['total_vector_count']:,}\")\n",
    "print(f\"   Namespaces: {list(stats['namespaces'].keys())}\")\n",
    "print(f\"   Ready for vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test E5 vector upsert (safety check)\n",
    "print(\"ğŸ§ª Testing single E5 vector upsert...\")\n",
    "\n",
    "test_vector = pinecone_vectors[0]\n",
    "\n",
    "# Test upsert to isolated namespace\n",
    "test_upsert = index.upsert(\n",
    "    vectors=[test_vector],\n",
    "    namespace=\"fino-test-e5\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Single upsert successful\")\n",
    "print(f\"   Upserted ID: {test_vector['id']}\")\n",
    "print(f\"   Response: {test_upsert}\")\n",
    "\n",
    "# Test retrieval (critical!)\n",
    "print(f\"\\nğŸ” Testing E5 retrieval...\")\n",
    "\n",
    "# Create test query embedding\n",
    "test_query_text = \"savings account minimum balance\"\n",
    "test_query_embedding = e5_embeddings.embed_query(test_query_text)\n",
    "\n",
    "# Query test namespace\n",
    "test_results = index.query(\n",
    "    vector=test_query_embedding,\n",
    "    top_k=3,\n",
    "    namespace=\"fino-test-e5\",\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Query returned {len(test_results['matches'])} matches:\")\n",
    "for i, match in enumerate(test_results['matches'], 1):\n",
    "    print(f\"\\n   [{i}] Score: {match['score']:.4f}\")\n",
    "    print(f\"       ID: {match['id']}\")\n",
    "    print(f\"       Policy: {match['metadata'].get('policy_id', 'N/A')}\")\n",
    "    print(f\"       Preview: {match['metadata'].get('text_preview', 'N/A')[:100]}...\")\n",
    "\n",
    "print(f\"\\nâœ… E5 vector upsert & retrieval working perfectly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full production batch upsert (E5)\n",
    "print(\"ğŸš€ PRODUCTION UPSERT: All E5 vectors to Pinecone\")\n",
    "print(f\"ğŸ“¤ {len(pinecone_vectors):,} vectors â†’ fino-policies namespace\")\n",
    "\n",
    "# Pinecone batch limits: 100MB / request, typically 1000 vectors safe\n",
    "batch_size = 500\n",
    "namespace = \"fino-policies-e5\"\n",
    "\n",
    "total_upserted = 0\n",
    "for batch_idx in range(0, len(pinecone_vectors), batch_size):\n",
    "    batch_end = min(batch_idx + batch_size, len(pinecone_vectors))\n",
    "    batch = pinecone_vectors[batch_idx:batch_end]\n",
    "    \n",
    "    # Upsert batch\n",
    "    response = index.upsert(\n",
    "        vectors=batch,\n",
    "        namespace=namespace\n",
    "    )\n",
    "    \n",
    "    total_upserted += len(batch)\n",
    "    progress = (batch_end / len(pinecone_vectors)) * 100\n",
    "    \n",
    "    print(f\"   Batch {(batch_idx // batch_size) + 1}: {len(batch)} vectors \"\n",
    "          f\"({progress:.1f}%) â†’ {response}\")\n",
    "\n",
    "print(f\"\\nâœ… UPSERT COMPLETE!\")\n",
    "print(f\"   Total vectors: {total_upserted:,}\")\n",
    "print(f\"   Namespace: {namespace}\")\n",
    "print(f\"   Model: E5-Large\")\n",
    "\n",
    "# Final index stats\n",
    "final_stats = index.describe_index_stats()\n",
    "print(f\"\\nğŸ“Š Final Pinecone stats:\")\n",
    "print(f\"   Total vectors: {final_stats['total_vector_count']:,}\")\n",
    "print(f\"   Namespaces: {list(final_stats['namespaces'].keys())}\")\n",
    "for ns, ns_stats in final_stats['namespaces'].items():\n",
    "    print(f\"      {ns}: {ns_stats['vector_count']:,} vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54252b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
